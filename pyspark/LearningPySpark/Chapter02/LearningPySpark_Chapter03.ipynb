{"nbformat":4,"nbformat_minor":0,"metadata":{"name":"Ch4 - DataFrames","notebookId":4341522646494009,"colab":{"name":"LearningPySpark_Chapter03.ipynb","provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0ZTA6Uw4OlHM"},"source":["## Learning PySpark\n","### Chapter 4: DataFrames\n","This notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrames."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thPxK320O4OW","executionInfo":{"status":"ok","timestamp":1633115015427,"user_tz":420,"elapsed":40231,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"2b9e4cc6-6d7f-430f-b289-4e96ec3698b3"},"source":["!pip install pyspark"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n","\u001b[K     |████████████████████████████████| 212.4 MB 67 kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 53.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=6bdfa87e729ebee2196933c9272378afedcdceb3d419468684b70e0a65a9ba46\n","  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.1.2\n"]}]},{"cell_type":"code","metadata":{"id":"CBtzzQmHO4IL","executionInfo":{"status":"ok","timestamp":1633115066619,"user_tz":420,"elapsed":7234,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}}},"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","sc=spark.sparkContext"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8q6nhp-lO4Bu","executionInfo":{"status":"ok","timestamp":1633115093297,"user_tz":420,"elapsed":23508,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"ba887794-1437-4b56-8c34-3f66c73bae10"},"source":["from google.colab import drive\n","drive.mount('/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"LtH5fC4AOlHa"},"source":["### Generate your own DataFrame\n","Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."]},{"cell_type":"code","metadata":{"id":"RFKofEDlOlHd","executionInfo":{"status":"ok","timestamp":1633115100368,"user_tz":420,"elapsed":514,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}}},"source":["# Generate our own JSON data \n","#   This way we don't have to access the file system yet.\n","stringJSONRDD = sc.parallelize((\"\"\" \n","  { \"id\": \"123\",\n","    \"name\": \"Katie\",\n","    \"age\": 19,\n","    \"eyeColor\": \"brown\"\n","  }\"\"\",\n","   \"\"\"{\n","    \"id\": \"234\",\n","    \"name\": \"Michael\",\n","    \"age\": 22,\n","    \"eyeColor\": \"green\"\n","  }\"\"\", \n","  \"\"\"{\n","    \"id\": \"345\",\n","    \"name\": \"Simone\",\n","    \"age\": 23,\n","    \"eyeColor\": \"blue\"\n","  }\"\"\")\n",")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hl8dXCtwOlHf","executionInfo":{"status":"ok","timestamp":1633115109550,"user_tz":420,"elapsed":5752,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"c42066a9-3ab5-4788-a59d-293848dd3977"},"source":["# Create DataFrame\n","swimmersJSON = spark.read.json(stringJSONRDD)\n","swimmersJSON.take(10)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n"," Row(age=22, eyeColor='green', id='234', name='Michael'),\n"," Row(age=23, eyeColor='blue', id='345', name='Simone')]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"9IdjQTi7OlHg","executionInfo":{"status":"ok","timestamp":1633115112875,"user_tz":420,"elapsed":594,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}}},"source":["# Create temporary table\n","swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imDy4S2WOlHk","executionInfo":{"status":"ok","timestamp":1633115120434,"user_tz":420,"elapsed":720,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"b9f810e2-e324-4eae-bbd5-e70ba799afbd"},"source":["# DataFrame API\n","swimmersJSON.show()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------+---+-------+\n","|age|eyeColor| id|   name|\n","+---+--------+---+-------+\n","| 19|   brown|123|  Katie|\n","| 22|   green|234|Michael|\n","| 23|    blue|345| Simone|\n","+---+--------+---+-------+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUOF5gxoOlHl","executionInfo":{"status":"ok","timestamp":1633115123313,"user_tz":420,"elapsed":408,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"ff2aa164-2ab2-401c-c751-cf59f05db265"},"source":["# SQL Query\n","spark.sql(\"select * from swimmersJSON\").collect()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n"," Row(age=22, eyeColor='green', id='234', name='Michael'),\n"," Row(age=23, eyeColor='blue', id='345', name='Simone')]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"LY_NA5GBOlHp"},"source":["#### Inferring the Schema Using Reflection\n","Note that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5onhWqg9OlHq","executionInfo":{"status":"ok","timestamp":1633115137336,"user_tz":420,"elapsed":129,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"97201ed7-6274-4932-a2a0-83beaf77905b"},"source":["# Print the schema\n","swimmersJSON.printSchema()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- age: long (nullable = true)\n"," |-- eyeColor: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- name: string (nullable = true)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"WG0mxORROlHr"},"source":["Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n","\n","But what if we want to programmatically specify the schema?"]},{"cell_type":"markdown","metadata":{"id":"nVVC-kscOlHs"},"source":["#### Programmatically Specifying the Schema\n","In this case, let's specify the schema for a `CSV` text file."]},{"cell_type":"code","metadata":{"id":"PNFCQZFaOlHs","executionInfo":{"status":"ok","timestamp":1633115141658,"user_tz":420,"elapsed":270,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}}},"source":["from pyspark.sql.types import *\n","\n","# Generate our own CSV data \n","#   This way we don't have to access the file system yet.\n","stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n","\n","# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n","schemaString = \"id name age eyeColor\"\n","schema = StructType([\n","    StructField(\"id\", LongType(), True),    \n","    StructField(\"name\", StringType(), True),\n","    StructField(\"age\", LongType(), True),\n","    StructField(\"eyeColor\", StringType(), True)\n","])\n","\n","# Apply the schema to the RDD and Create DataFrame\n","swimmers = spark.createDataFrame(stringCSVRDD, schema)\n","\n","# Creates a temporary view using the DataFrame\n","swimmers.createOrReplaceTempView(\"swimmers\")"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3Ztg78cOlHt","executionInfo":{"status":"ok","timestamp":1633115146846,"user_tz":420,"elapsed":130,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"b1a2cb3e-3e99-4a5d-9731-a41cec2253b9"},"source":["# Print the schema\n","#   Notice that we have redefined id as Long (instead of String)\n","swimmers.printSchema()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- id: long (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- age: long (nullable = true)\n"," |-- eyeColor: string (nullable = true)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"rbl-3DYzOlHw"},"source":["As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n","\n","Additional Resources include:\n","* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n","* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."]},{"cell_type":"markdown","metadata":{"id":"_hJ5O2XAOlHw"},"source":["####|| SparkSession\n","\n","Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`.\n","* Entry point for reading data\n","* Working with metadata\n","* Configuration\n","* Cluster resource management\n","\n","For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."]},{"cell_type":"markdown","metadata":{"id":"D2e_VPJwOlHx"},"source":["### Querying with SQL\n","With DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDy2kq0yOlHx","executionInfo":{"status":"ok","timestamp":1633115168546,"user_tz":420,"elapsed":482,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"a6ffd8bc-d9b9-49ea-82e7-df1480784dee"},"source":["# Execute SQL Query and return the data\n","spark.sql(\"select * from swimmers\").show()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---+--------+\n","| id|   name|age|eyeColor|\n","+---+-------+---+--------+\n","|123|  Katie| 19|   brown|\n","|234|Michael| 22|   green|\n","|345| Simone| 23|    blue|\n","+---+-------+---+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"7C3v6WULOlHx"},"source":["Let's get the row count:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FppIIg4WOlHy","executionInfo":{"status":"ok","timestamp":1633115171567,"user_tz":420,"elapsed":904,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"9f0ee10b-43a6-44cc-a5ad-cbc413466b49"},"source":["# Get count of rows in SQL\n","spark.sql(\"select count(1) from swimmers\").show()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+\n","|count(1)|\n","+--------+\n","|       3|\n","+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"BIcokzBjOlHy"},"source":["Note, you can make use of `%sql` within the notebook cells of a Databricks notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDMXWfWJOlH1","executionInfo":{"status":"ok","timestamp":1633115180026,"user_tz":420,"elapsed":585,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"67a6ed46-96d2-4946-d958-4a6d4b17ca08"},"source":["# Query id and age for swimmers with age = 22 via DataFrame API\n","swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+\n","| id|age|\n","+---+---+\n","|234| 22|\n","+---+---+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7wjmzvfOlH1","executionInfo":{"status":"ok","timestamp":1633115183265,"user_tz":420,"elapsed":485,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"516bf78f-f64c-40e6-990f-b98e038bba1d"},"source":["# Query id and age for swimmers with age = 22 via DataFrame API in another way\n","swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+\n","| id|age|\n","+---+---+\n","|234| 22|\n","+---+---+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbSM2YKtOlH2","executionInfo":{"status":"ok","timestamp":1633115185057,"user_tz":420,"elapsed":273,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"1508dc64-5fa8-420b-e6da-ab79f8fb71e6"},"source":["# Query id and age for swimmers with age = 22 in SQL\n","spark.sql(\"select id, age from swimmers where age = 22\").show()"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+\n","| id|age|\n","+---+---+\n","|234| 22|\n","+---+---+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"zxypSF_FOlH2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633115191482,"user_tz":420,"elapsed":478,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"173ded9d-8304-4d26-d0c4-169d0b331e15"},"source":["# Query name and eye color for swimmers with eye color starting with the letter 'b'\n","spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------+\n","|  name|eyeColor|\n","+------+--------+\n","| Katie|   brown|\n","|Simone|    blue|\n","+------+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"1DezhHaSOlH4"},"source":["### Querying with the DataFrame API\n","With DataFrames, you can start writing your queries using the DataFrame API"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3HP5kioOlH4","executionInfo":{"status":"ok","timestamp":1633115198377,"user_tz":420,"elapsed":288,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"31155b8c-fc82-43ef-bd63-d6b6267d9237"},"source":["# Show the values \n","swimmers.show()"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------+---+--------+\n","| id|   name|age|eyeColor|\n","+---+-------+---+--------+\n","|123|  Katie| 19|   brown|\n","|234|Michael| 22|   green|\n","|345| Simone| 23|    blue|\n","+---+-------+---+--------+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"79p9cE7ROlH4","executionInfo":{"status":"ok","timestamp":1633115201388,"user_tz":420,"elapsed":130,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"95123ff3-0fc7-4104-f3f3-5665dbf0eb1f"},"source":["# Using Databricks `display` command to view the data easier\n","display(swimmers)"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6naMcNrOlH5","executionInfo":{"status":"ok","timestamp":1633115202887,"user_tz":420,"elapsed":287,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"b549f285-5571-48e3-b9a0-4533bb5c0b39"},"source":["# Get count of rows\n","swimmers.count()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9x6VDhekOlH5","executionInfo":{"status":"ok","timestamp":1633115204830,"user_tz":420,"elapsed":443,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"b372bb2e-6b4d-44ea-d118-ff966f1a70ce"},"source":["# Get the id, age where age = 22\n","swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+\n","| id|age|\n","+---+---+\n","|234| 22|\n","+---+---+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhF53OICOlH5","executionInfo":{"status":"ok","timestamp":1633115206884,"user_tz":420,"elapsed":439,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"66788256-2275-442c-e005-89c15cf465ff"},"source":["# Get the name, eyeColor where eyeColor like 'b%'\n","swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------+\n","|  name|eyeColor|\n","+------+--------+\n","| Katie|   brown|\n","|Simone|    blue|\n","+------+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"y-GOzruAOlH6"},"source":["## On-Time Flight Performance\n","Query flight departure delays by State and City by joining the departure delay and join to the airport codes (to identify state and city)."]},{"cell_type":"markdown","metadata":{"id":"cKNOlyKdOlH6"},"source":["### DataFrame Queries\n","Let's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PNomJm0OlH7","executionInfo":{"status":"ok","timestamp":1633115215078,"user_tz":420,"elapsed":3026,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"dd162255-509e-4a52-c718-aeb5c6948b94"},"source":["# Set File Paths\n","flightPerfFilePath = \"/drive/My Drive/Colab Notebooks/pyspark/LearningPySpark/Learning-PySpark-master/Data/departuredelays.csv\"\n","airportsFilePath = \"/drive/My Drive/Colab Notebooks/pyspark/LearningPySpark/Learning-PySpark-master/Data/airport-codes-na.txt\"\n","\n","# Obtain Airports dataset\n","airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n","airports.createOrReplaceTempView(\"airports\")\n","\n","# Obtain Departure Delays dataset\n","flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n","flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n","\n","# Cache the Departure Delays dataset \n","flightPerf.cache()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsyAkX3JOlH7","executionInfo":{"status":"ok","timestamp":1633115230761,"user_tz":420,"elapsed":12350,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"551a1596-f49a-43c2-cdb3-014d9732664c"},"source":["# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n","spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------+--------+\n","|   City|origin|  Delays|\n","+-------+------+--------+\n","|Seattle|   SEA|159086.0|\n","|Spokane|   GEG| 12404.0|\n","|  Pasco|   PSC|   949.0|\n","+-------+------+--------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"gg2RV-8SOlH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633115236003,"user_tz":420,"elapsed":3050,"user":{"displayName":"Raj Iyengar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14194838542243042437"}},"outputId":"b97c910b-91e3-4f88-8f43-8223477a3fec"},"source":["# Query Sum of Flight Delays by State (for the US)\n","spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---------+\n","|State|   Delays|\n","+-----+---------+\n","|   SC|  80666.0|\n","|   AZ| 401793.0|\n","|   LA| 199136.0|\n","|   MN| 256811.0|\n","|   NJ| 452791.0|\n","|   OR| 109333.0|\n","|   VA|  98016.0|\n","| null| 397237.0|\n","|   RI|  30760.0|\n","|   WY|  15365.0|\n","|   KY|  61156.0|\n","|   NH|  20474.0|\n","|   MI| 366486.0|\n","|   NV| 474208.0|\n","|   WI| 152311.0|\n","|   ID|  22932.0|\n","|   CA|1891919.0|\n","|   CT|  54662.0|\n","|   NE|  59376.0|\n","|   MT|  19271.0|\n","+-----+---------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"57LEAd2XOlH9"},"source":["For more information, please refer to:\n","* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n","* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n","* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"]}]}